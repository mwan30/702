{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fac29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae2184",
   "metadata": {},
   "source": [
    "#### Homework Reflection 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda04b8a",
   "metadata": {},
   "source": [
    "1. Write some code that will use a simulation to estimate the standard deviation of the coefficient when there is heteroskedasticity.  Compare these standard errors to those found via statsmodels OLS or a similar linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdac24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation-based standard deviation of coefficient: 0.2825\n",
      "OLS standard error of coefficient: 0.1915\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 100\n",
    "n_simulations = 1000\n",
    "beta_true = 2.0\n",
    "intercept = 1.0\n",
    "\n",
    "X = np.random.normal(0, 1, n)\n",
    "X_with_const = sm.add_constant(X)\n",
    "\n",
    "# Function to simulate heteroskedasticity errors\n",
    "def generate_heteroskedastic_errors(X, scale = 1.0):\n",
    "    sigma = scale * (1 + np.abs(X)) # error variance increases with X\n",
    "    errors = np.random.normal(0, sigma, len(X))\n",
    "    return errors\n",
    "\n",
    "# Data simulation and coefficient estimation\n",
    "beta_estimates = []\n",
    "for _ in range(n_simulations):\n",
    "    errors = generate_heteroskedastic_errors(X)\n",
    "    y = intercept + beta_true * X + errors\n",
    "    model = sm.OLS(y, X_with_const).fit()\n",
    "    beta_estimates.append(model.params[1])\n",
    "\n",
    "# Simulation based standard deviation of the coefficient\n",
    "beta_estimates = np.array(beta_estimates)\n",
    "sim_std = np.std(beta_estimates, ddof = 1)\n",
    "\n",
    "# Fit OLS model on one dataset to get standard errors\n",
    "errors = generate_heteroskedastic_errors(X)\n",
    "y = intercept + beta_true * X + errors\n",
    "model = sm.OLS(y, X_with_const).fit()\n",
    "ols_se = model.bse[1]  \n",
    "\n",
    "print(f\"Simulation-based standard deviation of coefficient: {sim_std:.4f}\")\n",
    "print(f\"OLS standard error of coefficient: {ols_se:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd72f1",
   "metadata": {},
   "source": [
    "<font color = 'seagreen'> The standard error of the coefficient found from simulation with heteroskedasticity was 0.2825. This is a larger value than the standard error of the coefficient found via statsmodels OLS, which was 0.1915. This shows that OLS standard errors are unreliable under heteroskedasticity and that simulation-based approaches better reflect the true uncertainties in the coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc8154",
   "metadata": {},
   "source": [
    "2. Write some code that will use a simulation to estimate the standard deviation of the coefficient when errors are highly correlated / non-independent.\n",
    "Compare these standard errors to those found via statsmodels OLS or a similar linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ee6c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation-based standard deviation of coefficient (correlated errors): 0.1033\n",
      "OLS standard error of coefficient (correlated errors): 0.0717\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 100\n",
    "n_simulations = 1000\n",
    "beta_true = 2.0\n",
    "intercept = 1.0\n",
    "rho = 0.9\n",
    "sigma = 1.0\n",
    "\n",
    "X = np.random.normal(0, 1, n)\n",
    "X_with_const = sm.add_constant(X)\n",
    "\n",
    "# Function to generate correlated errors\n",
    "def generate_correlated_errors(n, rho, sigma):\n",
    "    errors = np.zeros(n)\n",
    "    errors[0] = np.random.normal(0, sigma)\n",
    "    for t in range(1, n):\n",
    "        errors[t] = rho * errors[t-1] + np.random.normal(0, sigma * np.sqrt(1 - rho**2))\n",
    "    return errors\n",
    "\n",
    "# Data simulation and coefficient estimation\n",
    "beta_estimates = []\n",
    "for _ in range(n_simulations):\n",
    "    errors = generate_correlated_errors(n, rho, sigma)\n",
    "    y = intercept + beta_true * X + errors\n",
    "    model = sm.OLS(y, X_with_const).fit()\n",
    "    beta_estimates.append(model.params[1])\n",
    "\n",
    "# Calculate simulation based std\n",
    "beta_estimates = np.array(beta_estimates)\n",
    "sim_std = np.std(beta_estimates, ddof = 1)\n",
    "\n",
    "# Fit OLS model on one dataset to get standard errors\n",
    "errors = generate_correlated_errors(n, rho, sigma)\n",
    "y = intercept + beta_true * X + errors\n",
    "model = sm.OLS(y, X_with_const).fit()\n",
    "ols_se = model.bse[1]\n",
    "\n",
    "print(f\"Simulation-based standard deviation of coefficient (correlated errors): {sim_std:.4f}\")\n",
    "print(f\"OLS standard error of coefficient (correlated errors): {ols_se:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44122646",
   "metadata": {},
   "source": [
    "<font color='seagreen'> Again, the simulation based standard deviation (0.1033) was larger than the OLS standard error of 0.0717. This is likely because OLS assumptions are violated when errors are highly correlated. The simulated data is also able to capture true variability because the simulation-based standard deviation is derived from repeatedly sampling data with correlated errors and estimating the coefficient. Overall, this indicates that the true uncertainty in the coefficient estimates is greater than what OLS assumes due to error correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c57e2",
   "metadata": {},
   "source": [
    "Show that if the correlation between coefficients is high enough, then the estimated standard deviation of the coefficient, using bootstrap errors, might not match that found by a full simulation of the Data Generating Process. (This can be fixed if you have a huge amount of data for the bootstrap simulation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26357f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Sample Size (n=100):\n",
      "  Full DGP Simulation Std: 0.2968\n",
      "  OLS Std Error: 0.2389\n",
      "  Bootstrap Std: 0.2293\n",
      "\n",
      "Large Sample Size (n=10000):\n",
      "  Full DGP Simulation Std: 0.0320\n",
      "  OLS Std Error: 0.0310\n",
      "  Bootstrap Std: 0.0318\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def run_simulation(n, n_simulations = 1000, n_boostrap = 1000, rho_pred = 0.95, rho_err = 0.9, sigma = 1.0):\n",
    "    beta_true = np.array([1.0, 2.0])\n",
    "    intercept = 1.0\n",
    "    \n",
    "    cov_matrix = np.array([[1.0, rho_pred], [rho_pred, 1.0]])\n",
    "    \n",
    "    def generate_correlated_errors(n, rho, sigma):\n",
    "        errors = np.zeros(n)\n",
    "        errors[0] = np.random.normal(0, sigma)\n",
    "        for t in range(1, n):\n",
    "            errors[t] = rho * errors[t-1] + np.random.normal(0, sigma * np.sqrt(1 - rho**2))\n",
    "        return errors\n",
    "    \n",
    "    # Full DGP simulation\n",
    "    beta1_dgp_estimates = []\n",
    "    for _ in range(n_simulations):\n",
    "        X = np.random.multivariate_normal(mean = [0, 0], cov = cov_matrix, size = n)\n",
    "        X_with_const = sm.add_constant(X)\n",
    "        errors = generate_correlated_errors(n, rho_err, sigma)\n",
    "        y = intercept + X.dot(beta_true) + errors\n",
    "        model = sm.OLS(y, X_with_const).fit()\n",
    "        beta1_dgp_estimates.append(model.params[1])\n",
    "    \n",
    "    dgp_std = np.std(beta1_dgp_estimates, ddof = 1)\n",
    "\n",
    "    # Generate one dataset for bootstrap and OLS\n",
    "    X = np.random.multivariate_normal(mean = [0, 0], cov = cov_matrix, size = n)\n",
    "    X_with_const = sm.add_constant(X)\n",
    "    errors = generate_correlated_errors(n, rho_err, sigma)\n",
    "    y = intercept + X.dot(beta_true) + errors\n",
    "    model = sm.OLS(y, X_with_const).fit()\n",
    "    ols_se = model.bse[1]\n",
    "    \n",
    "    # Bootstrap simulation\n",
    "    fitted_values = model.fittedvalues\n",
    "    residuals = model.resid\n",
    "    beta1_bootstrap_estimates = []\n",
    "    for _ in range(n_boostrap):\n",
    "        resampled_residuals = np.random.choice(residuals, size = n, replace = True)\n",
    "        y_bootstrap = fitted_values + resampled_residuals\n",
    "        model_bootstrap = sm.OLS(y_bootstrap, X_with_const).fit()\n",
    "        beta1_bootstrap_estimates.append(model_bootstrap.params[1])\n",
    "    \n",
    "    boot_std = np.std(beta1_bootstrap_estimates, ddof = 1)\n",
    "    \n",
    "    return dgp_std, ols_se, boot_std, beta1_dgp_estimates, beta1_bootstrap_estimates\n",
    "\n",
    "n_small = 100\n",
    "dgp_std_small, ols_se_small, boot_std_small, dgp_estimates_small, boot_estimates_small = run_simulation(n_small)\n",
    "print(f\"Small Sample Size (n={n_small}):\")\n",
    "print(f\"  Full DGP Simulation Std: {dgp_std_small:.4f}\")\n",
    "print(f\"  OLS Std Error: {ols_se_small:.4f}\")\n",
    "print(f\"  Bootstrap Std: {boot_std_small:.4f}\")\n",
    "\n",
    "n_large = 10000\n",
    "dgp_std_large, ols_se_large, boot_std_large, dgp_estimates_large, boot_estimates_large = run_simulation(n_large)\n",
    "print(f\"\\nLarge Sample Size (n={n_large}):\")\n",
    "print(f\"  Full DGP Simulation Std: {dgp_std_large:.4f}\")\n",
    "print(f\"  OLS Std Error: {ols_se_large:.4f}\")\n",
    "print(f\"  Bootstrap Std: {boot_std_large:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58a79a",
   "metadata": {},
   "source": [
    "<font color='seagreen'> Using bootstrap errors, it's seen that the estimated standard deviation of the coefficient doesn't make the standard deviation found using a full simulation of the Data Generating Process. From the small sample size data, the bootstrap standard deviation found was 0.2293 and the full DGP simulation standard deviation found was 0.2968. However, as mentioned, this was fixed after using a larger sample data for the bootstrap simulation. So, the bootstrap standard deviation from the larger sample was 0.0318 and the full DGP simulation standard deviation was 0.0320."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eed720",
   "metadata": {},
   "source": [
    "#### Homework Reflection 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061ad84",
   "metadata": {},
   "source": [
    "1. Construct a dataset for an event study where the value, derivative, and second derivative for a trend all change discontinuously (suddenly) after an event. Build a model that tries to decided whether the event is real (has a nonzero effect) using:  \n",
    "(a) only the value,  \n",
    "(b) the value, derivative, and second derivative.  \n",
    "Which of these models is better at detecting and/or quantifying the impact of the event? (What might \"better\" mean here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83abb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.896\n",
      "Model:                            OLS   Adj. R-squared:                  0.895\n",
      "Method:                 Least Squares   F-statistic:                     1141.\n",
      "Date:                Sat, 09 Aug 2025   Prob (F-statistic):          2.12e-194\n",
      "Time:                        10:09:46   Log-Likelihood:                -551.02\n",
      "No. Observations:                 400   AIC:                             1110.\n",
      "Df Residuals:                     396   BIC:                             1126.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3382      0.120      2.819      0.005       0.102       0.574\n",
      "x1             0.8246      0.033     24.684      0.000       0.759       0.890\n",
      "x2             0.0626      0.006      9.678      0.000       0.050       0.075\n",
      "x3             0.9137      0.193      4.737      0.000       0.535       1.293\n",
      "==============================================================================\n",
      "Omnibus:                        2.887   Durbin-Watson:                   2.102\n",
      "Prob(Omnibus):                  0.236   Jarque-Bera (JB):                2.742\n",
      "Skew:                           0.131   Prob(JB):                        0.254\n",
      "Kurtosis:                       3.310   Cond. No.                         51.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.897\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     685.7\n",
      "Date:                Sat, 09 Aug 2025   Prob (F-statistic):          6.90e-192\n",
      "Time:                        10:09:46   Log-Likelihood:                -549.74\n",
      "No. Observations:                 400   AIC:                             1111.\n",
      "Df Residuals:                     394   BIC:                             1135.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0854      0.202      0.423      0.673      -0.312       0.482\n",
      "x1             0.5361      0.187      2.873      0.004       0.169       0.903\n",
      "x2             0.0065      0.036      0.181      0.857      -0.064       0.078\n",
      "x3             1.1795      0.289      4.079      0.000       0.611       1.748\n",
      "x4             0.2564      0.267      0.960      0.338      -0.269       0.782\n",
      "x5             0.0642      0.052      1.241      0.215      -0.038       0.166\n",
      "==============================================================================\n",
      "Omnibus:                        2.960   Durbin-Watson:                   2.116\n",
      "Prob(Omnibus):                  0.228   Jarque-Bera (JB):                2.862\n",
      "Skew:                           0.124   Prob(JB):                        0.239\n",
      "Kurtosis:                       3.331   Cond. No.                         102.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Example fit (a): p-value =  0.0000, estimated level jump =  0.9137\n",
      "Example fit (b): p-value =  0.0000, estimated level jump =  1.1795\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "def generate_data(n = 200, sigma = 1.0, level_jump = 1.0, slope_change = 0.5, curv_change = 0.02):\n",
    "    t_left = np.linspace(-5, 0, n, endpoint=False)\n",
    "    t_right = np.linspace(0, 5, n, endpoint=False)\n",
    "    t = np.concatenate([t_left, t_right])\n",
    "    post = (t > 0).astype(float)\n",
    "    y_left = 0 + 0.5 * t_left + 0.01 * t_left ** 2\n",
    "    y_right = level_jump + (0.5 + slope_change) * t_right + (0.01 + curv_change) * t_right ** 2\n",
    "    y_det = np.concatenate([y_left, y_right])\n",
    "    errors = np.random.normal(0, sigma, len(t))\n",
    "    y = y_det + errors\n",
    "    return y, t, post\n",
    "\n",
    "# Fit model a\n",
    "def fit_model_a(y, t, post):\n",
    "    X_a = np.column_stack((np.ones(len(t)), t, t**2, post))\n",
    "    model_a = sm.OLS(y, X_a).fit()\n",
    "    print(model_a.summary())\n",
    "    p_value_a = model_a.pvalues[3]\n",
    "    delta0_a = model_a.params[3]\n",
    "    return p_value_a, delta0_a\n",
    "\n",
    "# Fit model b\n",
    "def fit_model_b(y, t, post):\n",
    "    post_t = post * t\n",
    "    post_t2 = post * t**2\n",
    "    X_b = np.column_stack((np.ones(len(t)), t, t**2, post, post_t, post_t2))\n",
    "    model_b = sm.OLS(y, X_b).fit()\n",
    "    print(model_b.summary())\n",
    "    R = np.array(([0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]))\n",
    "    f_test = model_b.f_test(R)\n",
    "    p_value_b = f_test.pvalue\n",
    "    delta0_b = model_b.params[3]\n",
    "    return p_value_b, delta0_b\n",
    "\n",
    "# Generate and fit dataset\n",
    "np.random.seed(42)\n",
    "y, t, post = generate_data()\n",
    "p_a, delta0_a = fit_model_a(y, t, post)\n",
    "p_b, delta0_b = fit_model_b(y, t, post)\n",
    "print(f\"Example fit (a): p-value = {p_a: .4f}, estimated level jump = {delta0_a: .4f}\")\n",
    "print(f\"Example fit (b): p-value = {p_b: .4f}, estimated level jump = {delta0_b: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7f2f5",
   "metadata": {},
   "source": [
    "<font color='seagreen'> After creating a dataset and modeled it under the two scenarios, I found that model B is the \"better\" model at detecting and quantifying the impact. Model B is generally better for both detection and quantification in realistic event  studies. It correctly specifies the data-generating process, allowing it to detect subtle effects in derivatives/curvatures and provides unbiased estimates of the impact. In practice, \"better\" depends on the context: if computational simplicity matters and only level jumps are expected, model A suffices; otherwise, model B is preferable for robust inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1590c1",
   "metadata": {},
   "source": [
    "2. Construct a dataset in which there are three groups whose values each increase discontinuously (suddenly) by the same amount at a shared event; they change in parallel over time, but they have different starting values. Create a model that combines group fixed effects with an event study, as suggested in the online reading. Explain what you did, how the model works, and how it accounts for both baseline differences and the common event effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b32f4e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.976e+29\n",
      "Date:                Sat, 09 Aug 2025   Prob (F-statistic):               0.00\n",
      "Time:                        10:46:29   Log-Likelihood:                 869.61\n",
      "No. Observations:                  30   AIC:                            -1729.\n",
      "Df Residuals:                      25   BIC:                            -1722.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        19.0000   3.36e-14   5.65e+14      0.000      19.000      19.000\n",
      "C(group)[T.B]    10.0000   3.06e-14   3.27e+14      0.000      10.000      10.000\n",
      "C(group)[T.C]    20.0000   3.06e-14   6.55e+14      0.000      20.000      20.000\n",
      "post             10.0000   4.87e-14   2.05e+14      0.000      10.000      10.000\n",
      "time              1.0000   8.31e-15    1.2e+14      0.000       1.000       1.000\n",
      "==============================================================================\n",
      "Omnibus:                       12.567   Durbin-Watson:                   0.101\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):                2.574\n",
      "Skew:                          -0.042   Prob(JB):                        0.276\n",
      "Kurtosis:                       1.568   Cond. No.                         26.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "groups = ['A', 'B', 'C']\n",
    "baselines = {'A': 20, 'B': 30, 'C': 40}\n",
    "time_periods = range(1, 11)\n",
    "event_time = 5\n",
    "jump = 10\n",
    "trend_slope = 1\n",
    "\n",
    "data = []\n",
    "for group in groups:\n",
    "    for t in time_periods:\n",
    "        y = baselines[group] + (t - 1) * trend_slope\n",
    "        if t >= event_time:\n",
    "            y += jump\n",
    "        post = 1 if t >= event_time else 0\n",
    "        \n",
    "        data.append({'group': group, 'time': t, 'post': post, 'y': y})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "model = smf.ols('y ~ C(group) + post + time', data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8de3e8",
   "metadata": {},
   "source": [
    "<font color='seagreen'> I constructed a synthetic panel dataset with 3 groups (A, B, C) observed over 10 time periods. Each group has a baseline (A: 20, B: 30, C: 40) and a linear trend of 1 unit per period. The event occurred at t = 5 where a 10-unit jump happened. The groups have different baseline levels (0, 5, 10 for groups 1, 2, 3, respectively), but they evolve in parallel where each has the same linear trend (slope = 0.5) before and after the event. There was no change in the trend at the event, only a level shift.  \n",
    "The C(group) term creates dummies for groups B and C, capturing their baseline differences relative to Group A. This ensures that the model accounts for the different starting values, isolating the event effect from these constant differences. The post variable estimates the common jump across all groups at t = 5. The coefficient of 10 reflects the 10-unit increase, applied equally to all groups. The time variable controls for the linear trend (1 unit per period), ensuring the event is not confounded with the gradual increase over time. The model assumes parallel trends across groups, which our data satisfies. The fixed effects and time trend ensure the jump is attributed to the event, not baseline differences or trends.  \n",
    "The group fixed effects absorb the constant differences in levels. This prevents baseline differences from biasing the event effect estimate. The post coefficient captures the shared 10-unit jump at t = 5, net of baseline differences and trends. Because the model includes group fixed effects, it isolates the event's impact across all groups. The parallel trends assumption is satisfied, where all groups have the same slope before and after the event. The model correctly attributes the jump to the event, not to divergent trends. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa7624",
   "metadata": {},
   "source": [
    "#### Homework Reflection 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a2740",
   "metadata": {},
   "source": [
    "Construct a dataset in which prior trends do not hold, and in which this makes the differences-in-differences come out wrong. Explain why the differences-in-differences estimate of the effect comes out higher or lower than the actual effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d69d4a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit_ID</th>\n",
       "      <th>Group</th>\n",
       "      <th>Year</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Control</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Control</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Control</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Control</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Control</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Control</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>Treatment</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unit_ID      Group  Year  Income\n",
       "0         1    Control     1     100\n",
       "1         1    Control     2     100\n",
       "2         1    Control     3     100\n",
       "3         2    Control     1     100\n",
       "4         2    Control     2     100\n",
       "5         2    Control     3     100\n",
       "6         3  Treatment     1     100\n",
       "7         3  Treatment     2     110\n",
       "8         3  Treatment     3     125\n",
       "9         4  Treatment     1     100\n",
       "10        4  Treatment     2     110\n",
       "11        4  Treatment     3     125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Unit_ID': [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
    "    'Group': ['Control', 'Control', 'Control', 'Control', 'Control', 'Control', 'Treatment', 'Treatment', 'Treatment', 'Treatment', 'Treatment', 'Treatment'],\n",
    "    'Year': [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "    'Income': [100, 100, 100, 100, 100, 100, 100, 110, 125, 100, 110, 125]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11329691",
   "metadata": {},
   "source": [
    "<font color='seagreen'> From this basic dataset example, the average pre-treatment income (Years 1-2) has a control of 100 and treatment of 105. Post-treatment income in Year 3 has a control of 100 and treatment of 125. The change for the control is 100 - 100 = 0 while the change for treatment is 125 - 105 = +20. Thus, the DiD estimate would be +20 - 0 = +20. However, the actual treatment effect is +5 (observed 125 minus counterfactual 120 for the treatment group in Year 3). The DiD estimate of +20 is higher than the true effect by +15. This bias occurs because the parallel trends assumption is violated: in the absence of treatment, the treatment group would not have followed the same trend as the control group. The treatment group has a pre-existing upward trend (+10 from Year 1 to 2, and would have continued +10 from Year 2 to 3 without treatment), while the control group is flat. The DiD method assumes that any differential change post-treatment is solely due to the treatment, but here it incorrectly attributes the treatment group's ongoing upward trend (+10 from Year 2 to 3, plus the prior differential embedded in the pre-average) to the treatment effect. As a result, the estimate captures both the true effect of +5 and the extraneous trend of +15, leading to an overestimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
