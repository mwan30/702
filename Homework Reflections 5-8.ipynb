{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2186cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import scipy.spatial.distance as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4533a",
   "metadata": {},
   "source": [
    "#### Homework Reflection 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b80f5",
   "metadata": {},
   "source": [
    "1. Draw a diagram for the following negative feedback loop:\n",
    "\n",
    "Sweating causes body temperature to decrease. High body temperature causes sweating. \n",
    "\n",
    "A negative feedback loop means that one thing increases another while the second thing decreases the first.\n",
    "\n",
    "Remember that we are using directed acyclic graphs where two things cannot directly cause each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4800b7",
   "metadata": {},
   "source": [
    "<font color='seagreen'> Creating a directed acyclic graph for a negative feedback loop seems to be contradictory, so the only way to create this graph without any cycles would be to have body heat as two separate nodes, which creates the following:\n",
    "\n",
    "![Negative Feedback Loop DAG](2743.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf06b5",
   "metadata": {},
   "source": [
    "2. Describe an example of a positive feedback loop. This means that one thing increases another while the second thing also increases the first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4932c",
   "metadata": {},
   "source": [
    "<font color='seagreen'> An example of positive feedback loop is with fruit ripening. In a basket of apples, when one apple begins to ripen, it releases ethylene gas through its skin. When exposed to this gas, the apples near this apple begin to ripen, causing more ethylene gas to be released. This in turn causes the rest of the apples in the basket to ripen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce957f",
   "metadata": {},
   "source": [
    "3. Draw a diagram for the following situation: \n",
    "\n",
    "Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.  \n",
    "Bears eat deer, decreasing their population.  \n",
    "Deer eat flowers, decreasing their population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50ca70",
   "metadata": {},
   "source": [
    "![DAG2](54910.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c436fb",
   "metadata": {},
   "source": [
    "Write a dataset that simulates this situation. (Show the code.) Include noise/randomness in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f65b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lightning</th>\n",
       "      <th>bears</th>\n",
       "      <th>deer</th>\n",
       "      <th>flowers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50.533103</td>\n",
       "      <td>153.638687</td>\n",
       "      <td>403.324563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35.993967</td>\n",
       "      <td>160.756433</td>\n",
       "      <td>471.901897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>41.140594</td>\n",
       "      <td>136.858898</td>\n",
       "      <td>474.416856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51.831757</td>\n",
       "      <td>185.484961</td>\n",
       "      <td>320.350699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>51.679371</td>\n",
       "      <td>175.396911</td>\n",
       "      <td>345.066384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lightning      bears        deer     flowers\n",
       "0          0  50.533103  153.638687  403.324563\n",
       "1          1  35.993967  160.756433  471.901897\n",
       "2          1  41.140594  136.858898  474.416856\n",
       "3          0  51.831757  185.484961  320.350699\n",
       "4          0  51.679371  175.396911  345.066384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = 1000 # sample size \n",
    "lightning = np.random.binomial(1, 0.3, n)  # 1 if storm, 0 otherwise\n",
    "\n",
    "# Bears decrease with lightning, plus noise\n",
    "bears = 50 - 10 * lightning + np.random.normal(0, 3, n)\n",
    "\n",
    "# Deer decrease with lightning and bears, plus noise\n",
    "deer = 200 - 20 * lightning - 0.5 * bears + np.random.normal(0, 15, n)\n",
    "\n",
    "# Flowers increase with lightning and decrease with deer, plus noise\n",
    "flowers = 500 + 120 * lightning - 0.8 * deer + np.random.normal(0, 30, n)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'lightning': lightning,\n",
    "    'bears': bears,\n",
    "    'deer': deer,\n",
    "    'flowers': flowers\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cca31",
   "metadata": {},
   "source": [
    "Identify a backdoor path with one or more confounders for the relationship between deer and flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98176db",
   "metadata": {},
   "source": [
    "<font color='seagreen'>\n",
    "\n",
    "1. Deer <-- Lightning --> Flowers  \n",
    "\n",
    "2. Deer <-- Bear <-- Lightning --> Flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa648ef",
   "metadata": {},
   "source": [
    "4. Draw a diagram for a situation of your own invention. The diagram should include at least four nodes, one confounder, and one collider. Be sure that it is acyclic (no loops). Which node would say is most like a treatment (X)? Which node is most like an outcome (Y)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038ca93",
   "metadata": {},
   "source": [
    "![Exercise on MH](65804.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a16dee",
   "metadata": {},
   "source": [
    "<font color='seagreen'> This DAG looks at the effect of exercise (X) on mental health (Y) where exercising would improve mental health. The confounder (stress) affects both exercise and mental health by decreasing both. Sleep quality, the collider, is affected by both exercise and mental health where exercising would improve sleep quality but poor mental health worsens sleep quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da81279",
   "metadata": {},
   "source": [
    "#### Homework Reflection 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5dcfe",
   "metadata": {},
   "source": [
    "1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference? (Hint: think of statistics here. Consider that only the most extreme item ends up being used to estimate the MTE. That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect. But there is nevertheless a problem.)  \n",
    "Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness. (Please explain/justify this answer, or give a different one if you can think of one.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e06d3",
   "metadata": {},
   "source": [
    "<font color='seagreen'> The possible answer involves estimating the treatment effect for each untreated unit by comparing it to a counterfactual prediction. Then, the unit with the maximum estimated treatment effect is selected as the estimate of the MTE. While this aligns with the idea behind MTE of identifying the unit that would benefit the most from treatment, it introduces a statistical problem: the unit with the largest observed effect is likely to be an outlier, not because it truly has the largest effect, but because of random noise in the data. This can cause extreme value bias where many estimates are computed, each with some level of random variation, and the largest value often inflated by that variation. Essentially, random fluctuation is being captured that happens to make that unit look extreme. Thus, finding the unit with the highest causal effect often turns into finding the unit with the highest noisy estimate, leading to an overestimate of the true maximum effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eccd559",
   "metadata": {},
   "source": [
    "2. Propose a solution that remedies this problem and write some code that implements your solution. It's very important here that you clearly explain what your solution will do.  \n",
    "Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.  \n",
    "(Either code this answer or choose a different one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2f370f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True MTE (max true effects)</th>\n",
       "      <th>Estimated MTE (max estimate)</th>\n",
       "      <th>Proposed Solution (90th percentile)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.705463</td>\n",
       "      <td>17.954193</td>\n",
       "      <td>9.728453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True MTE (max true effects)  Estimated MTE (max estimate)  \\\n",
       "0                    12.705463                     17.954193   \n",
       "\n",
       "   Proposed Solution (90th percentile)  \n",
       "0                             9.728453  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Underlying true effects with some noise\n",
    "true_effects = np.random.normal(5, 2, n)\n",
    "estimation_noise = np.random.normal(0, 3, n)\n",
    "estimated_effects = true_effects + estimation_noise\n",
    "\n",
    "# Finding the 90th percentile of the estimated effects as MTE\n",
    "mte_90th_percentile = np.percentile(estimated_effects, 90)\n",
    "\n",
    "summary = {\n",
    "    'True MTE (max true effects)': np.max(true_effects),\n",
    "    'Estimated MTE (max estimate)': np.max(estimated_effects),\n",
    "    'Proposed Solution (90th percentile)': mte_90th_percentile\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694e821",
   "metadata": {},
   "source": [
    "#### Homework Reflection 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d3fef",
   "metadata": {},
   "source": [
    "1. Create a linear regression model involving a confounder that is left out of the model. Show whether the true correlation between $X$ and $Y$ is overestimated, underestimated, or neither. Explain in words why this is the case for the given coefficients you have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a10a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>With Z</th>\n",
       "      <th>Without Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>2.989823</td>\n",
       "      <td>2.403799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>-1.457839</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>0.006134</td>\n",
       "      <td>0.042120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         With Z  Without Z\n",
       "X      2.989823   2.403799\n",
       "Z     -1.457839        NaN\n",
       "const  0.006134   0.042120"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Confounder Z\n",
    "Z = np.random.normal(0, 1, n)\n",
    "\n",
    "# Treatment X influenced by Z\n",
    "X = 2 * Z + np.random.normal(0, 1, n)\n",
    "\n",
    "# Outcome Y influenced by X and Z\n",
    "Y = 3 * X - 1.5 * Z + np.random.normal(0, 1, n)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'X': X,\n",
    "    'Y': Y,\n",
    "    'Z': Z\n",
    "})\n",
    "\n",
    "# Model 1: with confounder Z\n",
    "X_Z = sm.add_constant(data[['X', 'Z']])\n",
    "model_with_Z = sm.OLS(data['Y'], X_Z).fit()\n",
    "\n",
    "# Model 2: without confounder Z\n",
    "X_only = sm.add_constant(data[['X']])\n",
    "model_without_Z = sm.OLS(data['Y'], X_only).fit()\n",
    "\n",
    "# Coefficients\n",
    "coefficients = {\n",
    "    'With Z': model_with_Z.params,\n",
    "    'Without Z': model_without_Z.params\n",
    "}\n",
    "coefficients_df = pd.DataFrame(coefficients)\n",
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86abe34",
   "metadata": {},
   "source": [
    "<font color='seagreen'> With this model, Z positively influenced X where as Z negatively influenced Y. The coefficient of X with Z was 2.99 and without Z was 2.40. The negative influence of Z on Y gets incorrectly absorbed into the estimate of X's effect, leading to an underestimate of the true causal effect of X on Y. \n",
    "\n",
    "On the other hand, if the model was created where Z positively influenced both X and Y, the coefficient without Z would be greater than the coefficient with Z, causing an overestimation. Overall, the influence of Z on X and Y affects the bias direction. If Z's effect of X is positive or negative and Z's effect on Y is negative or positive, respectively, then the correlation will be underestimated. If Z's effect on X is positive or negative and Z's effect on Y is positive or negative, respectively, the correlation will be overestimated. \n",
    "\n",
    "This happens because when a confounder is left out, the regression model mistakes the confounder's effect for part of the treatment effect. Whether the estimate is too big or too small depends on the direction of the confounder's effect on both the treatment and the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe261821",
   "metadata": {},
   "source": [
    "2. Performe a linear regression analysis in which one of the coefficients is zero, e.g.  \n",
    "\n",
    "W = [noise]  \n",
    "X = [noise]  \n",
    "Y = 2 * X + [noise]  \n",
    "\n",
    "And compute the p-value of a coefficient - in this case, the coefficient of W.  \n",
    "(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)  \n",
    "If the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)  \n",
    "Run the analysis 1000 times and report the best (smallest) p-value.  \n",
    "If the p-value is less than 0.05, does this mean the coefficient acutally is nonzero? What is the problem with repeating the analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c22316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4933752408637956)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Simulate data\n",
    "W = np.random.normal(0, 1, n)\n",
    "X = np.random.normal(0, 1, n)\n",
    "Y = 2 * X + np.random.normal(0, 1, n)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'W': W,\n",
    "    'X': X,\n",
    "    'Y': Y\n",
    "})\n",
    "\n",
    "# Model with confounder W\n",
    "X_W = sm.add_constant(data[['X', 'W']])\n",
    "model_with_W = sm.OLS(data['Y'], X_W).fit()\n",
    "\n",
    "# Compute p-value of coefficient of W\n",
    "p_value_W = model_with_W.pvalues['W']\n",
    "p_value_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ede02fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.00019365846399438825)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the analysis 1000 times\n",
    "p_values = []\n",
    "for _ in range(1000):\n",
    "    W = np.random.normal(0, 1, n)\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    Y = 2 * X + np.random.normal(0, 1, n)\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'W': W,\n",
    "        'X': X,\n",
    "        'Y': Y\n",
    "    })\n",
    "\n",
    "    X_W = sm.add_constant(data[['X', 'W']])\n",
    "    model_with_W = sm.OLS(data['Y'], X_W).fit()\n",
    "    p_values.append(model_with_W.pvalues['W'])\n",
    "\n",
    "smallest_p_value = min(p_values)\n",
    "smallest_p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497d3db",
   "metadata": {},
   "source": [
    "<font color='seagreen'> After running the analysis 1000 times, the smallest p-value observed is 0.00019, which is less than 0.05. This means that W is statistically significant even though W has no true effect on Y. Despite this low p-value, the coefficient is not necessarily nonzero. A low p-value can happen by chance, especially when the analysis is run as many as 1000 times. Running the regression once revealed a p-value of 0.49, indicating that W is statistically insignificant. However, running it 1000 times is guaranteed to find at least one misleadingly low p-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ed211",
   "metadata": {},
   "source": [
    "#### Homework Reflection 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ecf9d",
   "metadata": {},
   "source": [
    "Include the code you used to solve the two coding quiz problems and write about the obstacles/challenges/insights you encountered while solving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce01eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.2743411898510133)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ATE with inverse probability weighting\n",
    "\n",
    "hw81 = pd.read_csv('homework_8.1.csv', index_col=0)\n",
    "\n",
    "X = hw81['X']\n",
    "Y = hw81['Y']\n",
    "Z = pd.DataFrame(hw81['Z'])\n",
    "\n",
    "# Predicting propensity scores using predict_proba\n",
    "model = LogisticRegression()\n",
    "model.fit(Z, X)\n",
    "prob = model.predict_proba(Z)[:, 1]\n",
    "\n",
    "# Calculating weights using inverse probability weighting\n",
    "weights = np.where(X == 1, 1/prob, 1/(1-prob))\n",
    "hw81['weights'] = weights\n",
    "\n",
    "# Calculating ATE using weighted average\n",
    "X_0 = hw81[hw81['X'] == 0]\n",
    "X_1 = hw81[hw81['X'] == 1]\n",
    "avg_treated = np.sum(X_1['Y'] * X_1['weights'])/np.sum(X_1['weights'])\n",
    "avg_untreated = np.sum(X_0['Y'] * X_0['weights'])/np.sum(X_0['weights'])\n",
    "ate = avg_treated - avg_untreated\n",
    "ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ea045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 3.4376789979126094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Z1    2.696224\n",
       "Z2    0.538155\n",
       "Name: 241, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matching using Mahalanobis distance\n",
    "\n",
    "hw82 = pd.read_csv('homework_8.2.csv', index_col=0)\n",
    "\n",
    "# Separating treated and untreated groups\n",
    "treated = hw82[hw82['X'] == 1].reset_index(drop=True)\n",
    "untreated = hw82[hw82['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Calculating the covariance matrix and its inverse using Z1 and Z2\n",
    "matrix = np.vstack((hw82['Z1'], hw82['Z2']))\n",
    "cov = np.cov(matrix)\n",
    "cov_inv = np.linalg.inv(cov)\n",
    "\n",
    "# Calculate Mahalanobis distance for each untreated unit\n",
    "matches = []\n",
    "for i, row in treated.iterrows():\n",
    "    treated_z = row[['Z1', 'Z2']].values\n",
    "    distances = untreated[['Z1', 'Z2']].apply(lambda x: dist.mahalanobis(treated_z, x.values, cov_inv), axis=1)\n",
    "    nearest_index = distances.idxmin()\n",
    "    matches.append((i, nearest_index))\n",
    "\n",
    "matched_pairs = pd.DataFrame([{'treated_index': t_idx, 'control_index': c_idx, 'treated_Y': treated.loc[t_idx,'Y'], 'control_Y': untreated.loc[c_idx, 'Y']} for t_idx, c_idx in matches]) \n",
    "\n",
    "# Calculate the Average Treatment Effect (ATE)\n",
    "matched_pairs['TE'] = matched_pairs['treated_Y'] - matched_pairs['control_Y']\n",
    "print(f'ATE: {matched_pairs[\"TE\"].mean()}')\n",
    "\n",
    "# Finding the worst match based on Mahalanobis distance\n",
    "matched_pairs['distance'] = matched_pairs.apply(lambda row: dist.mahalanobis(treated.loc[row['treated_index'], ['Z1', 'Z2']].values, untreated.loc[row['control_index'], ['Z1', 'Z2']].values, cov_inv), axis=1)\n",
    "worst_match = matched_pairs.loc[matched_pairs['distance'].idxmax()]\n",
    "worst_treated = treated.loc[worst_match['treated_index'], ['Z1', 'Z2']]\n",
    "worst_treated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91f2cf",
   "metadata": {},
   "source": [
    "<font color='seagreen'> The biggest challenge for me when solving for these two code quiz problems was figuring out how to take the theoretical information we learned during lectures and readings and apply it into code format. It required a lot of trial and error as well as searching the Internet for resources. From doing these two problems, I feel like I have a better understanding of the concepts that we learned this week and how to apply it to real data. I was also able to brush up on some numpy and pandas skills that I haven't used very often, reminding me the robust usage of these two packages. Overall, I think this code quiz was a really great exercise and test of our knowledge. Although it was challenging, it really helped me pinpoint any holes in my knowledge of these concepts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
